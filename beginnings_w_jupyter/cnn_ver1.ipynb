{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-906f2aac218f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m            \u001b[1;31m# working with, mainly resizing, images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m         \u001b[1;31m# dealing with arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m                  \u001b[1;31m# dealing with directories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;31m# mixing up or currently ordered data that might lead our network astray in training.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m \u001b[1;31m#from tqdm import tqdm      # a nice pretty percentage bar for tasks. Thanks to viewer Daniel BA1/4hler for this suggestion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2            # working with, mainly resizing, images\n",
    "import numpy as np         # dealing with arrays\n",
    "import os                  # dealing with directories\n",
    "from random import shuffle # mixing up or currently ordered data that might lead our network astray in training.\n",
    "from tqdm import tqdm_notebook #from tqdm import tqdm      # a nice pretty percentage bar for tasks. Thanks to viewer Daniel BA1/4hler for this suggestion\n",
    "from lxml import etree\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "\n",
    "#D:/cnntry/air_data/trainairplane/  #training pictures of airplane\n",
    "#D:/cnntry/air_data/annot_train    #annotation files of training airplane pictures\n",
    "\n",
    "#D:/cnntry/air_data/valairplane/\n",
    "#D:/cnntry/air_data/annot_val/\n",
    "\n",
    "\n",
    "TRAIN_dir = 'D:/cnntry/air_data/trainairplane'\n",
    "TRAIN_annot_dir = 'D:/cnntry/air_data/annot_train'\n",
    "TRAIN_DIR = 'D:\\cnntry\\\\train'\n",
    "\n",
    "#TEST_DIR = 'D:\\cnntry\\\\test'\n",
    "TEST_DIR ='andrej/datasets/cats-and-dogs/1/home/test'\n",
    "\n",
    "DATA_file= 'train_airdata.npy' \n",
    "IMG_SIZE = 50\n",
    "LR = 1e-3\n",
    "\n",
    "MODEL_NAME = 'cnnv1-{}-{}.model'.format(LR, 'idk') # just so we remember which saved model is which, sizes must match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_img(img):\n",
    "    word_label = img.split('.')[-3]\n",
    "    # conversion to one-hot array [cat,dog]\n",
    "    #                            [much cat, no dog]\n",
    "    if word_label == 'cat': return [1,0]\n",
    "    #                             [no cat, very doggo]\n",
    "    elif word_label == 'dog': return [0,1]\n",
    "\n",
    "\n",
    "def get_label(img):\n",
    "    annot_label = img.split('.')[0]+'.xml'           #xml file name\n",
    "    annot_xml = os.path.join(TRAIN_annot_dir,annot_label)  #xml file full path\n",
    "    \n",
    "    root = etree.parse(annot_xml).getroot()\n",
    "    #for child in root: print(child.tag, child.attrib)\n",
    "    count = sum(1 for _ in root.iter(\"object\")) #count of objects in xml file\n",
    "    a = random.getrandbits(1)\n",
    "    if count == 1 : return [1,0]  # if only 1 airplane\n",
    "    else: return [0,1]            # of more than one airplane\n",
    "    \n",
    "def create_data(DIR,savename):\n",
    "    training_data = []\n",
    "    for img in tqdm_notebook(os.listdir(DIR)):\n",
    "        label = get_label(img)\n",
    "        path = os.path.join(DIR,img)\n",
    "        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "        training_data.append([np.array(img),np.array(label)])\n",
    "    shuffle(training_data)\n",
    "    np.save(savename, training_data)\n",
    "    return training_data\n",
    "\n",
    "def create_train_data():\n",
    "    training_data = []\n",
    "    for img in tqdm_notebook(os.listdir(TRAIN_DIR)):\n",
    "        label = label_img(img)\n",
    "        path = os.path.join(TRAIN_DIR,img)\n",
    "        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "        training_data.append([np.array(img),np.array(label)])\n",
    "    shuffle(training_data)\n",
    "    np.save('train_data.npy', training_data)\n",
    "    return training_data\n",
    "\n",
    "def process_test_data():\n",
    "    testing_data = []\n",
    "    for img in tqdm(os.listdir(TEST_DIR)):\n",
    "        path = os.path.join(TEST_DIR,img)\n",
    "        img_num = img.split('.')[0]\n",
    "        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "        testing_data.append([np.array(img), img_num])\n",
    "        \n",
    "    shuffle(testing_data)\n",
    "    np.save('test_data.npy', testing_data)\n",
    "    return testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN_DIR = '/floyd/input/cats_and_dogs/train' #'/train/train.csv' \n",
    "#print(os.listdir(\"/floyd/input/cats_and_dogs\"))\n",
    "\n",
    "train_data = create_data(TRAIN_dir,DATA_file)\n",
    "#train_data = create_train_data()  \n",
    "#print(os.listdir(TRAIN_dir)[0].split('.')[0])\n",
    "#train_data = np.load('D:\\cnntry\\\\train_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('D:\\cnntry\\\\train_airdata.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#propert structure\n",
    "#leaky relu\n",
    "#input normalization\n",
    "#proper output\n",
    "#reading output\n",
    "#getting nice anchors https://machinethink.net/blog/object-detection/\n",
    "decay=0.0005\n",
    "\n",
    "tflearn.layers.conv.conv_2d (incoming, nb_filter, filter_size, strides=1, padding='same', \n",
    "                             activation='linear', bias=True, weights_init='uniform_scaling', \n",
    "                             bias_init='zeros', regularizer=None, weight_decay=0.001, trainable=True, \n",
    "                             restore=True, reuse=False, scope=None, name='Conv2D')\n",
    "\n",
    "convnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n",
    "\n",
    "filters=[16,32,64,128,256,512,1024]\n",
    "for fil in filters:\n",
    "    convnet= conv_2d(convnet, fil, 3, strides=1,padding='same',activation='LeakyReLU',\n",
    "                     regularizer=None,weight_decay=decay)                  \n",
    "    convnet= max_pool_2d(convnet, 2, strides=2)\n",
    "\n",
    "###########\n",
    "\n",
    "convnet= conv_2d(convnet, 256, 3, strides=1,padding='same',activation='LeakyReLU',regularizer=None,weight_decay=decay)\n",
    "convnet= conv_2d(convnet, 512, 3, strides=1,padding='same',activation='LeakyReLU',regularizer=None,weight_decay=decay)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#del convnet\n",
    "convnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n",
    "\n",
    "filters=[32,64,128,64,32]\n",
    "\n",
    "for i in filters:\n",
    "    convnet = conv_2d(convnet, i, 5, activation='LeakyReLU')\n",
    "    convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = fully_connected(convnet, 1024, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "\n",
    "convnet = fully_connected(convnet, 2, activation='softmax')\n",
    "convnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "\n",
    "model = tflearn.DNN(convnet, tensorboard_dir='log')\n",
    "\n",
    "\n",
    "\n",
    "if os.path.exists('\\log\\{}.meta'.format(MODEL_NAME)):\n",
    "    model.load(MODEL_NAME)\n",
    "    print('model loaded!')\n",
    "\n",
    "train = train_data[:-500]\n",
    "test = train_data[-500:]\n",
    "\n",
    "X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "Y = [i[1] for i in train]\n",
    "\n",
    "test_x = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "test_y = [i[1] for i in test]\n",
    "\n",
    "\n",
    "print(len(train))\n",
    "print(train[1][0].shape)\n",
    "print(X.shape)\n",
    "print(len(test_x))\n",
    "print(test_x[0].shape)\n",
    "print(X.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit({'input': X}, {'targets': Y}, n_epoch=1, validation_set=({'input': test_x}, {'targets': test_y}), \n",
    "    snapshot_step=500, show_metric=True, run_id=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\cnntry',\n",
       " 'C:\\\\Users\\\\Admin\\\\Anaconda3\\\\python37.zip',\n",
       " 'C:\\\\Users\\\\Admin\\\\Anaconda3\\\\DLLs',\n",
       " 'C:\\\\Users\\\\Admin\\\\Anaconda3\\\\lib',\n",
       " 'C:\\\\Users\\\\Admin\\\\Anaconda3',\n",
       " '',\n",
       " 'C:\\\\Users\\\\Admin\\\\Anaconda3\\\\lib\\\\site-packages',\n",
       " 'C:\\\\Users\\\\Admin\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\Users\\\\Admin\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\Users\\\\Admin\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin',\n",
       " 'C:\\\\Users\\\\Admin\\\\Anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions',\n",
       " 'C:\\\\Users\\\\Admin\\\\.ipython']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(10): print(random.getrandbits(1))\n",
    "#model.save(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tflearn.layers.conv.conv_2d (incoming, nb_filter, filter_size, strides=1, padding='same', \n",
    "                             activation='linear', bias=True, weights_init='uniform_scaling', \n",
    "                             bias_init='zeros', regularizer=None, weight_decay=0.001, trainable=True, \n",
    "                             restore=True, reuse=False, scope=None, name='Conv2D')\n",
    "Input= 4-D Tensor [batch, height, width, in_channels].\n",
    "Output= 4-D Tensor [batch, new height, new width, nb_filter].\n",
    "\n",
    "\n",
    "tflearn.layers.conv.max_pool_2d (incoming, kernel_size, strides=None, padding='same', name='MaxPool2D')\n",
    "Input= 4-D Tensor [batch, height, width, in_channels].\n",
    "Output= 4-D Tensor [batch, pooled height, pooled width, in_channels].\n",
    "\n",
    "\n",
    "tflearn.layers.core.fully_connected (incoming, n_units, activation='linear', \n",
    "                                     bias=True, weights_init='truncated_normal', bias_init='zeros', \n",
    "                                     regularizer=None, weight_decay=0.001, trainable=True, restore=True, \n",
    "                                     reuse=False, scope=None, name='FullyConnected')\n",
    "Input= (2+)-D Tensor [samples, input dim]. If not 2D, input will be flatten.\n",
    "Output= 2D Tensor [samples, n_units].\n",
    "\n",
    "\n",
    "tflearn.layers.estimator.regression (incoming, placeholder='default', optimizer='adam', \n",
    "                                     loss='categorical_crossentropy', metric='default', learning_rate=0.001, \n",
    "                                     dtype=tf.float32, batch_size=64, shuffle_batches=True, to_one_hot=False, \n",
    "                                     n_classes=None, trainable_vars=None, restore=True, op_name=None, \n",
    "                                     validation_monitors=None, validation_batch_size=None, name=None)\n",
    "                                     \n",
    "tflearn.activations.leaky_relu (x, alpha=0.1, name='LeakyReLU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
